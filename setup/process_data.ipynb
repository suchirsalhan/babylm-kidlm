{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d936f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/suchirsalhan/Library/Python/3.13/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_path = 'facebook/opt-350m'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7f5ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `climb` has been saved to /Users/suchirsalhan/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/suchirsalhan/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `climb`\n",
      "Requirement already satisfied: dotenv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from dotenv) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token REPLACE WITH TOKEN\n",
    "!pip3 install dotenv\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "\n",
    "# Load environment variables (make sure .env contains HF_TOKEN)\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "# Set the repo URL or name (must be under your HF account or organization)\n",
    "repo_name = \"Talking-Babies\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72a66ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (0.32.3)\n",
      "Requirement already satisfied: packaging in /Users/suchirsalhan/Library/Python/3.13/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/suchirsalhan/Library/Python/3.13/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/suchirsalhan/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: Jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from Jinja2) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Loading dataset...\n",
      "Training new tokenizer with 16k vocabulary size -- keeping all other settings the same\n",
      "Creating a generator over the KidLM corpus...\n",
      "\n",
      "\n",
      "\n",
      "Pushing tokenizer to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/suchirsalhan/Talking-Babies/commit/85bc004c6ceb4ea1f83d9bda2b3e7e20e44e3a6b', commit_message='Upload tokenizer', commit_description='', oid='85bc004c6ceb4ea1f83d9bda2b3e7e20e44e3a6b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/suchirsalhan/Talking-Babies', endpoint='https://huggingface.co', repo_type='model', repo_id='suchirsalhan/Talking-Babies'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install Jinja2\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load Hugging Face token from .env\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Load the base tokenizer (same as before)\n",
    "tokenizer_path = 'facebook/opt-350m'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Load HF dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"tafseer-nayeem/KidLM-corpus\", split=\"train\")\n",
    "\n",
    "# Define data iterator over text field\n",
    "def data_iterator():\n",
    "    print(\"Creating a generator over the KidLM corpus...\")\n",
    "    for example in dataset:\n",
    "        yield example[\"text\"]\n",
    "\n",
    "# Train a new tokenizer with vocab size 16,000\n",
    "print(\"Training new tokenizer with 16k vocabulary size -- keeping all other settings the same\")\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(data_iterator(), vocab_size=16000)\n",
    "\n",
    "# Push to hub\n",
    "print(\"Pushing tokenizer to Hugging Face Hub...\")\n",
    "new_tokenizer.push_to_hub(\"Talking-Babies\", use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618aa74",
   "metadata": {},
   "source": [
    "OPT Tokenizer for KidLM  â€“ if we want to do anything for fixed seq lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7da11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Talking-Babies/opt-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e50d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(examples, seq_len):\n",
    "    \"\"\"\n",
    "    Tokenizes and chunks text data to fixed-length sequences.\n",
    "    \n",
    "    Args:\n",
    "        examples: A batch of text examples from the dataset\n",
    "        seq_len: The length of the sequences to chunk the text into\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing chunked token sequences of length SEQ_LEN\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    # Process each text example in the batch\n",
    "    for text in examples['text']:\n",
    "        # Convert text to token IDs\n",
    "        _tokens = tokenizer.encode(text)\n",
    "        # Add EOS token to mark the end of each text example\n",
    "        _tokens.append(tokenizer.eos_token_id)\n",
    "        # Accumulate all tokens in a flat list\n",
    "        tokens.extend(_tokens)\n",
    "\n",
    "    # Split the accumulated tokens into chunks of SEQ_LEN\n",
    "    chunks = [tokens[i:i + seq_len] for i in range(0, len(tokens), seq_len)]\n",
    "    \n",
    "    # Discard the last chunk if it's shorter than SEQ_LEN to ensure uniform sequence length\n",
    "    if len(chunks[-1]) < seq_len:\n",
    "        chunks = chunks[:-1]\n",
    "        \n",
    "    return {'input_ids': chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "data_root_path = 'data/raw/train_100M'\n",
    "\n",
    "raw_data_list= []\n",
    "\n",
    "def raw_data_iterator(data_root_path):\n",
    "    for file in os.listdir(data_root_path):\n",
    "        print(\"Processing file: \", file)\n",
    "        with open(os.path.join(data_root_path, file), 'r') as f:\n",
    "            document = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    raw_data_list.append({'text': ' '.join(document)})\n",
    "                    document = []\n",
    "                else:\n",
    "                    document.append(line)\n",
    "\n",
    "data_iterator = raw_data_iterator(data_root_path)\n",
    "raw_dataset = Dataset.from_list(raw_data_list)\n",
    "raw_dataset = raw_dataset.shuffle(seed=420)\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "from functools import partial\n",
    "SINGLE_SHUFFLE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a584158",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_chunk_2048 = partial(tokenize_and_chunk, seq_len=2048)\n",
    "\n",
    "tokenized_dataset_2048 = raw_dataset.map(\n",
    "    tokenize_and_chunk_2048,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_2048 = tokenized_dataset_2048.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20469cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = 'data/processed/train_100M_2048'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_2048.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e66536",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_2048\"   \n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_2048_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_2048.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
